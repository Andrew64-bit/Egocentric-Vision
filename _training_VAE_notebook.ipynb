{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAE for RGB Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedDataset\n",
    "\n",
    "from models import FC_VAE\n",
    "from train_vae import train, evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")\n",
    "\n",
    "features_file = \"saved_features/saved_feat_I3D_25_dense_D1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FeaturesExtendedDataset(features_file,'train')\n",
    "train_loader_rgb = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "\n",
    "model = FC_VAE(dim_input=1024, nz=64)\n",
    "model.to(DEVICE)\n",
    "print(f'Initial model device: {model.device}')\n",
    "\n",
    "# Create Optimizer & Scheduler objects\n",
    "optimizer = Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train(model, optimizer, EPOCHS, DEVICE, train_loader_rgb, train_loader_rgb, BATCH_SIZE, scheduler)\n",
    "\n",
    "torch.save(model.state_dict(), f'./saved_models/VAE_RGB/final_VAE_RGB_epoch_{EPOCHS}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FeaturesExtendedDataset(features_file,'train')\n",
    "test_dataset = FeaturesExtendedDataset(features_file,'test')\n",
    "train_loader_rgb = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_loader_rgb = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "model = FC_VAE(dim_input=1024, nz=64)\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'./saved_models/VAE_RGB/final_VAE_RGB_epoch_50.pth'))\n",
    "\n",
    "reconstructed, originals = evaluate(model, DEVICE, train_loader_rgb)\n",
    "reconstructed2, originals2 = evaluate(model, DEVICE, test_loader_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAE for EMG Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedEMGDataset\n",
    "\n",
    "from models import FC_VAE\n",
    "from train_vae import train_emg, evaluate_emg\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 0.003\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")\n",
    "\n",
    "LSTM_features_file_train = \"saved_features/NEW_EMG_Emb_LSTM_25_dense_D1_train.pkl\"\n",
    "LSTM_features_file_test = \"saved_features/NEW_EMG_Emb_LSTM_25_dense_D1_test.pkl\"\n",
    "STAT_features_file_train = \"saved_features/EMG_Emb_Stat_25_dense_D1_train.pkl\"\n",
    "STAT_features_file_test = \"saved_features/EMG_Emb_Stat_25_dense_D1_test.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FeaturesExtendedEMGDataset(LSTM_features_file_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True, multiprocessing_context='fork' if torch.backends.mps.is_available() else None)\n",
    "\n",
    "for i in train_loader:\n",
    "    print(i[\"features\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (rgb_batch_idx, x) in enumerate(train_loader):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:04, 57.93it/s]\n",
      "2024-06-11 21:59:54 LOG INFO \tEpoch, 1, \tAverage Loss: , 0.4263950584716694\n",
      "280it [00:04, 58.70it/s]\n",
      "2024-06-11 21:59:59 LOG INFO \tEpoch, 2, \tAverage Loss: , 0.4987638844410792\n",
      "280it [00:04, 59.40it/s]\n",
      "2024-06-11 22:00:03 LOG INFO \tEpoch, 3, \tAverage Loss: , 0.35013523910154576\n",
      "280it [00:04, 58.75it/s]\n",
      "2024-06-11 22:00:08 LOG INFO \tEpoch, 4, \tAverage Loss: , 0.3265098321475222\n",
      "280it [00:04, 58.71it/s]\n",
      "2024-06-11 22:00:13 LOG INFO \tEpoch, 5, \tAverage Loss: , 0.32591598746650535\n",
      "280it [00:04, 58.59it/s]\n",
      "2024-06-11 22:00:18 LOG INFO \tEpoch, 6, \tAverage Loss: , 0.32472302911529405\n",
      "280it [00:04, 59.22it/s]\n",
      "2024-06-11 22:00:23 LOG INFO \tEpoch, 7, \tAverage Loss: , 0.3250541244783709\n",
      "280it [00:04, 58.66it/s]\n",
      "2024-06-11 22:00:28 LOG INFO \tEpoch, 8, \tAverage Loss: , 0.32557127258897256\n",
      "280it [00:04, 57.42it/s]\n",
      "2024-06-11 22:00:33 LOG INFO \tEpoch, 9, \tAverage Loss: , 0.32512725400988773\n",
      "280it [00:04, 59.68it/s]\n",
      "2024-06-11 22:00:37 LOG INFO \tEpoch, 10, \tAverage Loss: , 0.3245349899735502\n",
      "280it [00:04, 58.54it/s]\n",
      "2024-06-11 22:00:42 LOG INFO \tEpoch, 11, \tAverage Loss: , 0.32530534925693677\n",
      "280it [00:04, 59.02it/s]\n",
      "2024-06-11 22:00:47 LOG INFO \tEpoch, 12, \tAverage Loss: , 0.3251666233584445\n",
      "280it [00:04, 58.54it/s]\n",
      "2024-06-11 22:00:52 LOG INFO \tEpoch, 13, \tAverage Loss: , 0.3246420338269203\n",
      "280it [00:04, 58.42it/s]\n",
      "2024-06-11 22:00:57 LOG INFO \tEpoch, 14, \tAverage Loss: , 0.3251425068438267\n",
      "280it [00:04, 59.03it/s]\n",
      "2024-06-11 22:01:02 LOG INFO \tEpoch, 15, \tAverage Loss: , 0.32458352362970727\n",
      "280it [00:04, 58.28it/s]\n",
      "2024-06-11 22:01:07 LOG INFO \tEpoch, 16, \tAverage Loss: , 0.3251035851042544\n",
      "280it [00:04, 57.39it/s]\n",
      "2024-06-11 22:01:11 LOG INFO \tEpoch, 17, \tAverage Loss: , 0.3241570134610472\n",
      "280it [00:04, 59.06it/s]\n",
      "2024-06-11 22:01:16 LOG INFO \tEpoch, 18, \tAverage Loss: , 0.32466520479876937\n",
      "280it [00:04, 59.00it/s]\n",
      "2024-06-11 22:01:21 LOG INFO \tEpoch, 19, \tAverage Loss: , 0.32436552386076645\n",
      "280it [00:04, 58.95it/s]\n",
      "2024-06-11 22:01:26 LOG INFO \tEpoch, 20, \tAverage Loss: , 0.3239636281672131\n",
      "280it [00:04, 58.93it/s]\n",
      "2024-06-11 22:01:31 LOG INFO \tEpoch, 21, \tAverage Loss: , 0.325189161311341\n",
      "280it [00:04, 58.75it/s]\n",
      "2024-06-11 22:01:36 LOG INFO \tEpoch, 22, \tAverage Loss: , 0.3246702108545543\n",
      "280it [00:04, 59.31it/s]\n",
      "2024-06-11 22:01:41 LOG INFO \tEpoch, 23, \tAverage Loss: , 0.3248988951966968\n",
      "280it [00:04, 59.35it/s]\n",
      "2024-06-11 22:01:45 LOG INFO \tEpoch, 24, \tAverage Loss: , 0.3247532249832239\n",
      "280it [00:04, 58.20it/s]\n",
      "2024-06-11 22:01:50 LOG INFO \tEpoch, 25, \tAverage Loss: , 0.3250990601583621\n",
      "280it [00:04, 59.48it/s]\n",
      "2024-06-11 22:01:55 LOG INFO \tEpoch, 26, \tAverage Loss: , 0.3252532034711812\n",
      "280it [00:04, 60.32it/s]\n",
      "2024-06-11 22:02:00 LOG INFO \tEpoch, 27, \tAverage Loss: , 0.32527075121746696\n",
      "280it [00:04, 58.51it/s]\n",
      "2024-06-11 22:02:05 LOG INFO \tEpoch, 28, \tAverage Loss: , 0.3251802708326061\n",
      "280it [00:04, 58.41it/s]\n",
      "2024-06-11 22:02:10 LOG INFO \tEpoch, 29, \tAverage Loss: , 0.32575518454969143\n",
      "280it [00:04, 59.78it/s]\n",
      "2024-06-11 22:02:14 LOG INFO \tEpoch, 30, \tAverage Loss: , 0.3251847278553739\n",
      "280it [00:04, 58.30it/s]\n",
      "2024-06-11 22:02:19 LOG INFO \tEpoch, 31, \tAverage Loss: , 0.3250228659012839\n",
      "280it [00:04, 59.08it/s]\n",
      "2024-06-11 22:02:24 LOG INFO \tEpoch, 32, \tAverage Loss: , 0.32418918728454565\n",
      "280it [00:04, 59.03it/s]\n",
      "2024-06-11 22:02:29 LOG INFO \tEpoch, 33, \tAverage Loss: , 0.32487617198070745\n",
      "280it [00:04, 58.88it/s]\n",
      "2024-06-11 22:02:34 LOG INFO \tEpoch, 34, \tAverage Loss: , 0.32496808549409273\n",
      "280it [00:04, 59.54it/s]\n",
      "2024-06-11 22:02:38 LOG INFO \tEpoch, 35, \tAverage Loss: , 0.32524459309689036\n",
      "280it [00:04, 59.21it/s]\n",
      "2024-06-11 22:02:43 LOG INFO \tEpoch, 36, \tAverage Loss: , 0.32498941898986855\n",
      "280it [00:04, 57.53it/s]\n",
      "2024-06-11 22:02:48 LOG INFO \tEpoch, 37, \tAverage Loss: , 0.3252409708729568\n",
      "280it [00:04, 58.85it/s]\n",
      "2024-06-11 22:02:53 LOG INFO \tEpoch, 38, \tAverage Loss: , 0.32485048693194185\n",
      "280it [00:04, 59.00it/s]\n",
      "2024-06-11 22:02:58 LOG INFO \tEpoch, 39, \tAverage Loss: , 0.3242989940482015\n",
      "280it [00:04, 58.09it/s]\n",
      "2024-06-11 22:03:03 LOG INFO \tEpoch, 40, \tAverage Loss: , 0.32519676042286727\n",
      "280it [00:04, 59.57it/s]\n",
      "2024-06-11 22:03:08 LOG INFO \tEpoch, 41, \tAverage Loss: , 0.32464860459809664\n",
      "280it [00:04, 58.09it/s]\n",
      "2024-06-11 22:03:13 LOG INFO \tEpoch, 42, \tAverage Loss: , 0.3231789785767755\n",
      "280it [00:04, 59.53it/s]\n",
      "2024-06-11 22:03:17 LOG INFO \tEpoch, 43, \tAverage Loss: , 0.32491526508363344\n",
      "280it [00:04, 59.48it/s]\n",
      "2024-06-11 22:03:22 LOG INFO \tEpoch, 44, \tAverage Loss: , 0.32511654677784146\n",
      "280it [00:04, 58.38it/s]\n",
      "2024-06-11 22:03:27 LOG INFO \tEpoch, 45, \tAverage Loss: , 0.3252151816762904\n",
      "280it [00:04, 58.93it/s]\n",
      "2024-06-11 22:03:32 LOG INFO \tEpoch, 46, \tAverage Loss: , 0.32480446333366053\n",
      "280it [00:04, 58.98it/s]\n",
      "2024-06-11 22:03:37 LOG INFO \tEpoch, 47, \tAverage Loss: , 0.3237297289928014\n",
      "280it [00:04, 57.84it/s]\n",
      "2024-06-11 22:03:42 LOG INFO \tEpoch, 48, \tAverage Loss: , 0.3252538615924483\n",
      "280it [00:04, 59.43it/s]\n",
      "2024-06-11 22:03:46 LOG INFO \tEpoch, 49, \tAverage Loss: , 0.32499062767966674\n",
      "280it [00:04, 59.68it/s]\n",
      "2024-06-11 22:03:51 LOG INFO \tEpoch, 50, \tAverage Loss: , 0.32324913433856434\n"
     ]
    }
   ],
   "source": [
    "model = FC_VAE(dim_input=256, nz=64)\n",
    "model.to(DEVICE)\n",
    "print(f'Initial model device: {model.device}')\n",
    "\n",
    "# Create Optimizer & Scheduler objects\n",
    "optimizer = Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_emg(model, optimizer, EPOCHS, DEVICE, train_loader, train_loader, BATCH_SIZE, scheduler)\n",
    "\n",
    "torch.save(model.state_dict(), f'./saved_models/VAE_EMG_LSTM_NEW/NEW_final_VAE_EMG_STAT_epoch_{EPOCHS}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:02, 98.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:01, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0088\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FeaturesExtendedEMGDataset(LSTM_features_file_train)\n",
    "test_dataset = FeaturesExtendedEMGDataset(LSTM_features_file_test)\n",
    "train_loader_emg = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_loader_emg = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "model = FC_VAE(dim_input=256, nz=64)\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'./saved_models/VAE_EMG_LSTM_NEW/NEW_final_VAE_EMG_STAT_epoch_50.pth'))\n",
    "\n",
    "reconstructed, originals = evaluate_emg(model, DEVICE, train_loader_emg, train_loader_emg)\n",
    "reconstructed2, originals2 = evaluate_emg(model, DEVICE, test_loader_emg, test_loader_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Training [ RGB --> EMG ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedDataset, FeaturesExtendedEMGDataset, ActionNetEmgRgbDataset\n",
    "from models import I3D\n",
    "from models import EMG_Feature_Extractor\n",
    "from utils.args import args\n",
    "from omegaconf import OmegaConf\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "from models import FC_VAE, LSTM_Emb_Classifier, EMG_Feature_Extractor\n",
    "from train_vae import train_tuning, evaluate_tuning\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 22:26:28 LOG INFO Loading Kinetics weights I3D\n",
      "2024-06-11 22:26:28 LOG INFO  * Skipping Logits weight for 'logits.conv3d.weight'\n",
      "2024-06-11 22:26:28 LOG INFO  * Skipping Logits weight for 'logits.conv3d.bias'\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------- I3D ----------------------\n",
    "# CONFIGURATION FOR I3D\n",
    "conf_args = OmegaConf.load('configs/I3D_save_feat.yaml')\n",
    "args = OmegaConf.merge(args, conf_args)\n",
    "\n",
    "model_rgb = I3D(20, \"RGB\", args.models['RGB'], **args.models['RGB'].kwargs)\n",
    "train_augmentations, test_augmentations = model_rgb.get_augmentation('RGB')\n",
    "model_rgb.to(\"cpu\")\n",
    "\n",
    "# ---------------------- LSTM ----------------------\n",
    "# Parametri del modello\n",
    "input_dim = 16\n",
    "hidden_dim = 300\n",
    "embedding_dim = 256\n",
    "output_dim = 20  # Definisci il numero di classi\n",
    "\n",
    "model_emg = LSTM_Emb_Classifier(input_dim=input_dim, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_class=output_dim)\n",
    "model_emg.load_state_dict(torch.load(f'./saved_models/LSTM_Emb_Classifier/NEW_final_LSTM_Emb_25_epoch_10.pth'))\n",
    "model_emg.to(DEVICE)\n",
    "\n",
    "# ---------------------- DATASET ----------------------\n",
    "# DATASET 25 FRAMES PER CLIP AND 1 SAMPLE PER BATCH\n",
    "num_frames = 25\n",
    "num_clips = 1\n",
    "batch_size = 1\n",
    "dataset = ActionNetEmgRgbDataset('train', num_frames, num_clips, True, './action-net', 'action-net/saved_emg', \"action-net/saved_RGB\", 2, train_augmentations)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False)\n",
    "len(data_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION ACTION NET s04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "embeddings_rgb = []\n",
    "embeddings_emg = []\n",
    "\n",
    "\n",
    "model_rgb.train(False)\n",
    "with torch.no_grad():\n",
    "    for (idx, (emg,rgb,l)) in enumerate(data_loader):\n",
    "        e = emg.reshape(batch_size, num_clips, num_frames, -1)  #(num_batch, num_clips, num_frames, num_features)\n",
    "        # print(e)                  # torch.Size([1, 1, 25, 16])\n",
    "        emg_input = e[0].float() # torch.Size([25, 16])\n",
    "        emg_input = emg_input.to(DEVICE)\n",
    "\n",
    "        batch, _, height, width = rgb.shape\n",
    "        rgb_reshape = rgb.reshape(1, num_clips, num_frames, -1, height, width)\n",
    "        rgb_permute = rgb_reshape.permute(1, 0, 3, 2, 4, 5)\n",
    "        rgb_input = rgb_permute[0].to('cpu')    # CLIP\n",
    "        # print(rgb_permute.shape)   torch.Size([1, 1, 3 (RGB), 25, 224, 224])\n",
    "        # print(l)                   tensor([16])\n",
    "        # ---------------------- RGB EXTRACTION ----------------------\n",
    "        output_rgb, feat_rgb = model_rgb(rgb_input)\n",
    "        feat_rgb = feat_rgb[\"features\"]\n",
    "        sample_rgb = feat_rgb[0]      # torch.Size([1, 1024])\n",
    "        embeddings_rgb.append(sample_rgb)\n",
    "\n",
    "        # ---------------------- EMG EXTRACTION ----------------------\n",
    "        outputs_emg, feat_emg = model_emg(emg_input)\n",
    "        sample_emg = feat_emg[0]      # torch.Size([64])\n",
    "        embeddings_emg.append(sample_emg)\n",
    "\n",
    "        # ---------------------- EMG STAT EXTRACTION ----------------------\n",
    "        #embeddings = EMG_Feature_Extractor(emg_input[0])\n",
    "        #embeddings_emg.append(embeddings)\n",
    "\n",
    "print(len(embeddings_emg))\n",
    "print(len(embeddings_emg))\n",
    "\n",
    "features_emg = \"saved_features/NEW_FINE_TUNING_emg_LSTM_s04_TRAIN.pkl\"\n",
    "features_rgb = \"saved_features/NEW_FINE_TUNING_rgb_s04_TRAIN.pkl\"\n",
    "\n",
    "with open(features_rgb, 'wb') as f:\n",
    "    pickle.dump(embeddings_rgb, f)\n",
    "\n",
    "with open(features_emg, 'wb') as f:\n",
    "    pickle.dump(embeddings_emg, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 0.0001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 50\n",
    "GAMMA = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB dataset size: 19\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.loaders import FeaturesTuningDataset\n",
    "\n",
    "rgb = FeaturesTuningDataset(\"./saved_features/NEW_FINE_TUNING_rgb_s04_TRAIN.pkl\")\n",
    "emg = FeaturesTuningDataset(\"./saved_features/NEW_FINE_TUNING_emg_LSTM_s04_TRAIN.pkl\")\n",
    "rgb_test = FeaturesTuningDataset(\"./saved_features/NEW_FINE_TUNING_rgb_s04_TEST.pkl\")\n",
    "emg_test = FeaturesTuningDataset(\"./saved_features/NEW_FINE_TUNING_emg_LSTM_s04_TEST.pkl\")\n",
    "print(f'RGB dataset size: {len(rgb_test)}')\n",
    "\n",
    "loader_rgb = DataLoader(rgb, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False, pin_memory=False)\n",
    "loader_emg = DataLoader(emg, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False, pin_memory=False)\n",
    "loader_rgb_test = DataLoader(rgb_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False, pin_memory=False)\n",
    "loader_emg_test = DataLoader(emg_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False, pin_memory=False)\n",
    "\n",
    "# print device loaders\n",
    "for (rgb_batch_idx, x) in enumerate(loader_rgb):\n",
    "    print(x.device)  # result --> cpu\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 22:37:18 LOG INFO \tEpoch, 1, \tAverage Loss: , 0.8024037688970566\n",
      "2024-06-11 22:37:18 LOG INFO \tEpoch, 2, \tAverage Loss: , 0.7427804488688707\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 3, \tAverage Loss: , 0.7411344680935145\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 4, \tAverage Loss: , 0.7384763456881046\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 5, \tAverage Loss: , 0.736753336340189\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 6, \tAverage Loss: , 0.7353636845946312\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 7, \tAverage Loss: , 0.73434037566185\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 8, \tAverage Loss: , 0.7338790915906429\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 9, \tAverage Loss: , 0.7334365762770176\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 10, \tAverage Loss: , 0.7331637397408486\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 11, \tAverage Loss: , 0.7331827685236931\n",
      "2024-06-11 22:37:19 LOG INFO \tEpoch, 12, \tAverage Loss: , 0.7328770682215691\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 13, \tAverage Loss: , 0.732866282761097\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 14, \tAverage Loss: , 0.7327185396105051\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 15, \tAverage Loss: , 0.7326300073415041\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 16, \tAverage Loss: , 0.732540863007307\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 17, \tAverage Loss: , 0.7329502683132887\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 18, \tAverage Loss: , 0.7323569525033236\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 19, \tAverage Loss: , 0.7324925489723683\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 20, \tAverage Loss: , 0.7324112217873335\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 21, \tAverage Loss: , 0.7323787696659565\n",
      "2024-06-11 22:37:20 LOG INFO \tEpoch, 22, \tAverage Loss: , 0.7321742318570614\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 23, \tAverage Loss: , 0.7321881502866745\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 24, \tAverage Loss: , 0.7325070485472679\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 25, \tAverage Loss: , 0.7321009065955877\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 26, \tAverage Loss: , 0.7321696553379298\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 27, \tAverage Loss: , 0.732173477485776\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 28, \tAverage Loss: , 0.7320084642618895\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 29, \tAverage Loss: , 0.7321023914963007\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 30, \tAverage Loss: , 0.731971500813961\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 31, \tAverage Loss: , 0.7322570588439703\n",
      "2024-06-11 22:37:21 LOG INFO \tEpoch, 32, \tAverage Loss: , 0.7321988448500634\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 33, \tAverage Loss: , 0.7316513635218144\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 34, \tAverage Loss: , 0.7317165162414312\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 35, \tAverage Loss: , 0.7330354683101177\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 36, \tAverage Loss: , 0.7320108223706484\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 37, \tAverage Loss: , 0.7318984527140856\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 38, \tAverage Loss: , 0.7317229211330414\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 39, \tAverage Loss: , 0.7316287137567997\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 40, \tAverage Loss: , 0.7317394394427538\n",
      "2024-06-11 22:37:22 LOG INFO \tEpoch, 41, \tAverage Loss: , 0.7313097290694713\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 42, \tAverage Loss: , 0.7309153273701667\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 43, \tAverage Loss: , 0.7312222223728895\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 44, \tAverage Loss: , 0.7317342657595873\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 45, \tAverage Loss: , 0.7326976966112853\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 46, \tAverage Loss: , 0.7316323034465313\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 47, \tAverage Loss: , 0.7317032307386399\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 48, \tAverage Loss: , 0.7313925258815288\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 49, \tAverage Loss: , 0.7312261503189802\n",
      "2024-06-11 22:37:23 LOG INFO \tEpoch, 50, \tAverage Loss: , 0.7309910099953413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0036\n"
     ]
    }
   ],
   "source": [
    "from train_vae import loss_function\n",
    "from utils.logger import setup_logger\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from train_vae import train_tuning, evaluate_tuning\n",
    "\n",
    "# LSTM EMG dimension is 64 \n",
    "# STAT EMG dimension is 112\n",
    "model_finetune = FC_VAE(dim_input=1024, nz=64, dim_output=256)\n",
    "model_finetune.to(DEVICE)\n",
    "\n",
    "# Carica i pesi del modello RGB per l'encoder\n",
    "checkpoint_rgb = torch.load('./saved_models/VAE_RGB/final_VAE_RGB_epoch_50.pth', map_location=DEVICE)\n",
    "# Rimuovi il prefisso 'encoder.' dalle chiavi dello state_dict\n",
    "checkpoint_rgb = {k.replace('encoder.', ''): v for k, v in checkpoint_rgb.items() if 'encoder' in k}\n",
    "model_finetune.encoder.load_state_dict(checkpoint_rgb)\n",
    "\n",
    "# Carica i pesi del modello EMG per il decoder\n",
    "checkpoint_emg = torch.load('./saved_models/VAE_EMG_LSTM_NEW/NEW_final_VAE_EMG_STAT_epoch_50.pth', map_location=DEVICE)\n",
    "checkpoint_emg = {k.replace('decoder.', ''): v for k, v in checkpoint_emg.items() if 'decoder' in k}\n",
    "model_finetune.decoder.load_state_dict(checkpoint_emg)\n",
    "\n",
    "# Create Optimizer & Scheduler objects\n",
    "optimizer = Adam(model_finetune.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_tuning(model_finetune, optimizer, EPOCHS, DEVICE, loader_rgb, loader_emg, BATCH_SIZE, scheduler)\n",
    "evaluate_tuning(model_finetune, DEVICE, loader_rgb_test, loader_emg_test)\n",
    "\n",
    "torch.save(model_finetune.state_dict(), f'./saved_models/VAE_Fine_Tuninng/VAE_RGB_to_EMG_epoch_{EPOCHS}.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
