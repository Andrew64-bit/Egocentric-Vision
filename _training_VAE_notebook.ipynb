{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAE for RGB Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedDataset\n",
    "\n",
    "from models import FC_VAE\n",
    "from train_vae import train, evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")\n",
    "\n",
    "features_file = \"saved_features/saved_feat_I3D_25_dense_D1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FeaturesExtendedDataset(features_file,'train')\n",
    "train_loader_rgb = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "\n",
    "model = FC_VAE(dim_input=1024, nz=64)\n",
    "model.to(DEVICE)\n",
    "print(f'Initial model device: {model.device}')\n",
    "\n",
    "# Create Optimizer & Scheduler objects\n",
    "optimizer = Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train(model, optimizer, EPOCHS, DEVICE, train_loader_rgb, train_loader_rgb, BATCH_SIZE, scheduler)\n",
    "\n",
    "torch.save(model.state_dict(), f'./saved_models/VAE_RGB/final_VAE_RGB_epoch_{EPOCHS}.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FeaturesExtendedDataset(features_file,'train')\n",
    "test_dataset = FeaturesExtendedDataset(features_file,'test')\n",
    "train_loader_rgb = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_loader_rgb = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "model = FC_VAE(dim_input=1024, nz=64)\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'./saved_models/VAE_RGB/final_VAE_RGB_epoch_50.pth'))\n",
    "\n",
    "reconstructed, originals = evaluate(model, DEVICE, train_loader_rgb)\n",
    "reconstructed2, originals2 = evaluate(model, DEVICE, test_loader_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training VAE for EMG Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedEMGDataset\n",
    "\n",
    "from models import FC_VAE\n",
    "from train_vae import train_emg, evaluate_emg\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")\n",
    "\n",
    "LSTM_features_file_train = \"saved_features/EMG_Emb_LSTM_25_dense_D1_train.pkl\"\n",
    "LSTM_features_file_test = \"saved_features/EMG_Emb_LSTM_25_dense_D1_test.pkl\"\n",
    "STAT_features_file_train = \"saved_features/EMG_Emb_Stat_25_dense_D1_train.pkl\"\n",
    "STAT_features_file_test = \"saved_features/EMG_Emb_Stat_25_dense_D1_test.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FeaturesExtendedEMGDataset(LSTM_features_file_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True, multiprocessing_context='fork' if torch.backends.mps.is_available() else None)\n",
    "\n",
    "for i in train_loader:\n",
    "    print(i[\"features\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (rgb_batch_idx, x) in enumerate(train_loader):\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FC_VAE(dim_input=112, nz=64)\n",
    "model.to(DEVICE)\n",
    "print(f'Initial model device: {model.device}')\n",
    "\n",
    "# Create Optimizer & Scheduler objects\n",
    "optimizer = Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_emg(model, optimizer, EPOCHS, DEVICE, train_loader, train_loader, BATCH_SIZE, scheduler)\n",
    "\n",
    "torch.save(model.state_dict(), f'./saved_models/VAE_EMG_STAT/final_VAE_EMG_STAT_epoch_{EPOCHS}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "280it [00:01, 154.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [00:00, 39.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0955\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FeaturesExtendedEMGDataset(STAT_features_file_train)\n",
    "test_dataset = FeaturesExtendedEMGDataset(STAT_features_file_test)\n",
    "train_loader_emg = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "test_loader_emg = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "model = FC_VAE(dim_input=112, nz=64)\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(f'./saved_models/VAE_EMG_STAT/final_VAE_EMG_STAT_epoch_50.pth'))\n",
    "\n",
    "reconstructed, originals = evaluate_emg(model, DEVICE, train_loader_emg, train_loader_emg)\n",
    "reconstructed2, originals2 = evaluate_emg(model, DEVICE, test_loader_emg, test_loader_emg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Training [ RGB --> EMG ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedDataset, FeaturesExtendedEMGDataset, ActionNetEmgRgbDataset\n",
    "from models import I3D\n",
    "from models import EMG_Feature_Extractor\n",
    "from utils.args import args\n",
    "from omegaconf import OmegaConf\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "from models import FC_VAE, LSTM_Emb_Classifier, EMG_Feature_Extractor\n",
    "from train_vae import train_tuning, evaluate_tuning\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:08:56 LOG INFO Loading Kinetics weights I3D\n",
      "2024-06-10 16:08:56 LOG INFO  * Skipping Logits weight for 'logits.conv3d.weight'\n",
      "2024-06-10 16:08:56 LOG INFO  * Skipping Logits weight for 'logits.conv3d.bias'\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- I3D ----------------------\n",
    "# CONFIGURATION FOR I3D\n",
    "conf_args = OmegaConf.load('configs/I3D_save_feat.yaml')\n",
    "args = OmegaConf.merge(args, conf_args)\n",
    "\n",
    "model_rgb = I3D(20, \"RGB\", args.models['RGB'], **args.models['RGB'].kwargs)\n",
    "train_augmentations, test_augmentations = model_rgb.get_augmentation('RGB')\n",
    "model_rgb.to(\"cpu\")\n",
    "\n",
    "# ---------------------- LSTM ----------------------\n",
    "# Parametri del modello\n",
    "input_dim = 16\n",
    "hidden_dim = 128\n",
    "embedding_dim = 64\n",
    "output_dim = 20  # Definisci il numero di classi\n",
    "\n",
    "model_emg = LSTM_Emb_Classifier(input_dim=input_dim, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_class=output_dim)\n",
    "model_emg.load_state_dict(torch.load(f'./saved_models/LSTM_Emb_Classifier/final_LSTM_Emb_epoch_40.pth'))\n",
    "model_emg.to(DEVICE)\n",
    "\n",
    "# ---------------------- DATASET ----------------------\n",
    "# DATASET 25 FRAMES PER CLIP AND 1 SAMPLE PER BATCH\n",
    "num_frames = 25\n",
    "num_clips = 1\n",
    "batch_size = 1\n",
    "dataset = ActionNetEmgRgbDataset('train', num_frames, num_clips, True, './action-net', 'action-net/saved_emg', \"action-net/saved_RGB\", 2, train_augmentations)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION ACTION NET s04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "embeddings_rgb = []\n",
    "embeddings_emg = []\n",
    "\n",
    "\n",
    "model_rgb.train(False)\n",
    "with torch.no_grad():\n",
    "    for (idx, (emg,rgb,l)) in enumerate(data_loader):\n",
    "        e = emg.reshape(batch_size, num_clips, num_frames, -1)  #(num_batch, num_clips, num_frames, num_features)\n",
    "        # print(e)                  # torch.Size([1, 1, 25, 16])\n",
    "        emg_input = e[0].float() # torch.Size([25, 16])\n",
    "        emg_input = emg_input.to(DEVICE)\n",
    "\n",
    "        batch, _, height, width = rgb.shape\n",
    "        rgb_reshape = rgb.reshape(1, num_clips, num_frames, -1, height, width)\n",
    "        rgb_permute = rgb_reshape.permute(1, 0, 3, 2, 4, 5)\n",
    "        rgb_input = rgb_permute[0].to('cpu')    # CLIP\n",
    "        # print(rgb_permute.shape)   torch.Size([1, 1, 3 (RGB), 25, 224, 224])\n",
    "        # print(l)                   tensor([16])\n",
    "        # ---------------------- RGB EXTRACTION ----------------------\n",
    "        output_rgb, feat_rgb = model_rgb(rgb_input)\n",
    "        feat_rgb = feat_rgb[\"features\"]\n",
    "        sample_rgb = feat_rgb[0]      # torch.Size([1, 1024])\n",
    "        embeddings_rgb.append(sample_rgb)\n",
    "\n",
    "        # ---------------------- EMG EXTRACTION ----------------------\n",
    "        #outputs_emg, feat_emg = model_emg(emg_input)\n",
    "        #sample_emg = feat_emg[0]      # torch.Size([64])\n",
    "        #embeddings_emg.append(sample_emg)\n",
    "\n",
    "        # ---------------------- EMG STAT EXTRACTION ----------------------\n",
    "        # embeddings = EMG_Feature_Extractor(emg_input[0])\n",
    "        # embeddings_emg.append(embeddings)\n",
    "\n",
    "print(len(embeddings_emg))\n",
    "#print(len(embeddings_emg))\n",
    "\n",
    "#features_emg = \"saved_features/FINE_TUNING_emg_STAT_s04.pkl\"\n",
    "features_rgb = \"saved_features/FINE_TUNING_rgb_s04.pkl\"\n",
    "\n",
    "with open(features_rgb, 'wb') as f:\n",
    "    pickle.dump(embeddings_rgb, f)\n",
    "\n",
    "#with open(features_emg, 'wb') as f:\n",
    "    #pickle.dump(embeddings_emg, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LR = 0.0001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB dataset size: 186\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.loaders import FeaturesTuningDataset\n",
    "\n",
    "rgb = FeaturesTuningDataset(\"./saved_features/FINE_TUNING_rgb_s04.pkl\")\n",
    "emg = FeaturesTuningDataset(\"./saved_features/FINE_TUNING_emg_STAT_s04.pkl\")\n",
    "print(f'RGB dataset size: {len(rgb)}')\n",
    "\n",
    "loader_rgb = DataLoader(rgb, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True, pin_memory=False)\n",
    "loader_emg = DataLoader(emg, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True, pin_memory=False)\n",
    "\n",
    "# print device loaders\n",
    "for (rgb_batch_idx, x) in enumerate(loader_rgb):\n",
    "    print(x.device)  # result --> cpu\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 1, \tAverage Loss: , 3.77432119846344\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 2, \tAverage Loss: , 3.738607108592987\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 3, \tAverage Loss: , 3.5968282222747803\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 4, \tAverage Loss: , 3.693473160266876\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 5, \tAverage Loss: , 3.6377466917037964\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 6, \tAverage Loss: , 3.4457544684410095\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 7, \tAverage Loss: , 3.432223081588745\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 8, \tAverage Loss: , 3.4242323637008667\n",
      "2024-06-10 16:13:29 LOG INFO \tEpoch, 9, \tAverage Loss: , 3.4463340640068054\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 10, \tAverage Loss: , 3.3542668223381042\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 11, \tAverage Loss: , 3.4337971210479736\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 12, \tAverage Loss: , 3.3531784415245056\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 13, \tAverage Loss: , 3.4155396223068237\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 14, \tAverage Loss: , 3.4277154207229614\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 15, \tAverage Loss: , 3.374748408794403\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 16, \tAverage Loss: , 3.385727822780609\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 17, \tAverage Loss: , 3.2886136770248413\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 18, \tAverage Loss: , 3.438038170337677\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 19, \tAverage Loss: , 3.429361879825592\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 20, \tAverage Loss: , 3.3456060886383057\n",
      "2024-06-10 16:13:30 LOG INFO \tEpoch, 21, \tAverage Loss: , 3.4195075035095215\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 22, \tAverage Loss: , 3.298782706260681\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 23, \tAverage Loss: , 3.2280136942863464\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 24, \tAverage Loss: , 3.3326135873794556\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 25, \tAverage Loss: , 3.340708076953888\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 26, \tAverage Loss: , 3.4424569606781006\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 27, \tAverage Loss: , 3.307748794555664\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 28, \tAverage Loss: , 3.3398948311805725\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 29, \tAverage Loss: , 3.405210256576538\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 30, \tAverage Loss: , 3.5229077339172363\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 31, \tAverage Loss: , 3.4222185611724854\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 32, \tAverage Loss: , 3.442816138267517\n",
      "2024-06-10 16:13:31 LOG INFO \tEpoch, 33, \tAverage Loss: , 3.4072362780570984\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 34, \tAverage Loss: , 3.352844774723053\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 35, \tAverage Loss: , 3.353195011615753\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 36, \tAverage Loss: , 3.3181238770484924\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 37, \tAverage Loss: , 3.4739585518836975\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 38, \tAverage Loss: , 3.2565993070602417\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 39, \tAverage Loss: , 3.4709948897361755\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 40, \tAverage Loss: , 3.282969653606415\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 41, \tAverage Loss: , 3.369099736213684\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 42, \tAverage Loss: , 3.4422836899757385\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 43, \tAverage Loss: , 3.3583292961120605\n",
      "2024-06-10 16:13:32 LOG INFO \tEpoch, 44, \tAverage Loss: , 3.247804582118988\n",
      "2024-06-10 16:13:33 LOG INFO \tEpoch, 45, \tAverage Loss: , 3.336747109889984\n",
      "2024-06-10 16:13:33 LOG INFO \tEpoch, 46, \tAverage Loss: , 3.411075532436371\n",
      "2024-06-10 16:13:33 LOG INFO \tEpoch, 47, \tAverage Loss: , 3.3575774431228638\n",
      "2024-06-10 16:13:33 LOG INFO \tEpoch, 48, \tAverage Loss: , 3.299540340900421\n",
      "2024-06-10 16:13:33 LOG INFO \tEpoch, 49, \tAverage Loss: , 3.3864471912384033\n",
      "2024-06-10 16:13:33 LOG INFO \tEpoch, 50, \tAverage Loss: , 3.233292877674103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0716\n"
     ]
    }
   ],
   "source": [
    "from train_vae import loss_function\n",
    "from utils.logger import setup_logger\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from train_vae import train_tuning, evaluate_tuning\n",
    "\n",
    "# LSTM EMG dimension is 64 \n",
    "# STAT EMG dimension is 112\n",
    "model_finetune = FC_VAE(dim_input=1024, nz=64, dim_output=112)\n",
    "model_finetune.to(DEVICE)\n",
    "\n",
    "# Carica i pesi del modello RGB per l'encoder\n",
    "checkpoint_rgb = torch.load('./saved_models/VAE_RGB/final_VAE_RGB_epoch_50.pth', map_location=DEVICE)\n",
    "# Rimuovi il prefisso 'encoder.' dalle chiavi dello state_dict\n",
    "checkpoint_rgb = {k.replace('encoder.', ''): v for k, v in checkpoint_rgb.items() if 'encoder' in k}\n",
    "model_finetune.encoder.load_state_dict(checkpoint_rgb)\n",
    "\n",
    "# Carica i pesi del modello EMG per il decoder\n",
    "checkpoint_emg = torch.load('./saved_models/VAE_EMG_STAT/final_VAE_EMG_STAT_epoch_50.pth', map_location=DEVICE)\n",
    "checkpoint_emg = {k.replace('decoder.', ''): v for k, v in checkpoint_emg.items() if 'decoder' in k}\n",
    "model_finetune.decoder.load_state_dict(checkpoint_emg)\n",
    "\n",
    "# Create Optimizer & Scheduler objects\n",
    "optimizer = Adam(model_finetune.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_tuning(model_finetune, optimizer, EPOCHS, DEVICE, loader_rgb, loader_emg, BATCH_SIZE, scheduler)\n",
    "evaluate_tuning(model_finetune, DEVICE, loader_rgb, loader_emg)\n",
    "\n",
    "torch.save(model_finetune.state_dict(), f'./saved_models/VAE_Fine_Tuninng/VAE_RGB_to_EMG_STAT_epoch_{EPOCHS}.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
