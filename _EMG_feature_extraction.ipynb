{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for EMG signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using LSTM --->  TRAINING + EVALUATION + EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Emb_Classifier, EMG_Feature_Extractor\n",
    "from utils.loaders import ActionNetEmgDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.logger import logger\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from utils.loaders import FeaturesExtendedEMGDataset\n",
    "from models import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training [ + EVALUATION ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 17:01:29 LOG INFO ------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.008\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 20\n",
    "GAMMA = 0.1\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    logger.info(\"------ USING APPLE SILICON GPU ------\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "2024-06-09 17:01:30 LOG INFO Model: LSTM_Emb_Classifier(\n",
      "  (lstm): LSTM(16, 128, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=20, bias=True)\n",
      ")\n",
      "2024-06-09 17:01:30 LOG INFO len train_dataset: 1795\n",
      "2024-06-09 17:01:30 LOG INFO len train_loader: 56\n"
     ]
    }
   ],
   "source": [
    "# Parametri del modello\n",
    "input_dim = 16\n",
    "hidden_dim = 128\n",
    "embedding_dim = 64\n",
    "output_dim = 20  # Definisci il numero di classi\n",
    "\n",
    "# Inizializzazione del modello, della loss function e dell'ottimizzatore\n",
    "model = LSTM_Emb_Classifier(input_dim=input_dim, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_class=output_dim)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_dataset = ActionNetEmgDataset('train', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2) # Inserisci il path al dataset di training\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) # Inserisci il dataloader per il training\n",
    "\n",
    "val_dataset = ActionNetEmgDataset('test', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "logger.info(f\"Model: {model}\")\n",
    "logger.info(f\"len train_dataset: {len(train_dataset)}\")\n",
    "logger.info(f\"len train_loader: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.reshape(BATCH_SIZE, 5, 25, -1)\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            for i in range(5):\n",
    "                x_t = x[i].float().to(device)\n",
    "                outputs, embeddings = model(x_t)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00,  8.10it/s]2024-06-09 17:01:36 LOG INFO [11/56]\n",
      "2024-06-09 17:01:36 LOG INFO [11/56]\n",
      "2024-06-09 17:01:36 LOG INFO [11/56]\n",
      "2024-06-09 17:01:36 LOG INFO [11/56]\n",
      "2024-06-09 17:01:36 LOG INFO [11/56]\n",
      "17it [00:01, 22.57it/s]2024-06-09 17:01:36 LOG INFO [22/56]\n",
      "2024-06-09 17:01:36 LOG INFO [22/56]\n",
      "2024-06-09 17:01:36 LOG INFO [22/56]\n",
      "2024-06-09 17:01:36 LOG INFO [22/56]\n",
      "2024-06-09 17:01:36 LOG INFO [22/56]\n",
      "32it [00:01, 33.10it/s]2024-06-09 17:01:36 LOG INFO [33/56]\n",
      "2024-06-09 17:01:36 LOG INFO [33/56]\n",
      "2024-06-09 17:01:36 LOG INFO [33/56]\n",
      "2024-06-09 17:01:36 LOG INFO [33/56]\n",
      "2024-06-09 17:01:36 LOG INFO [33/56]\n",
      "43it [00:01, 41.01it/s]2024-06-09 17:01:36 LOG INFO [44/56]\n",
      "2024-06-09 17:01:36 LOG INFO [44/56]\n",
      "2024-06-09 17:01:36 LOG INFO [44/56]\n",
      "2024-06-09 17:01:36 LOG INFO [44/56]\n",
      "2024-06-09 17:01:36 LOG INFO [44/56]\n",
      "53it [00:01, 42.85it/s]2024-06-09 17:01:37 LOG INFO [55/56]\n",
      "2024-06-09 17:01:37 LOG INFO [55/56]\n",
      "2024-06-09 17:01:37 LOG INFO [55/56]\n",
      "2024-06-09 17:01:37 LOG INFO [55/56]\n",
      "2024-06-09 17:01:37 LOG INFO [55/56]\n",
      "56it [00:04, 11.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_val,(x, y) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[1;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(BATCH_SIZE, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/multiprocessing/popen_fork.py:44\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = [0.0, 0]\n",
    "        for i_val,(x, y) in tqdm(enumerate(train_loader)):\n",
    "            x = x.reshape(BATCH_SIZE, 5, 25, -1)\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            y = y.to(DEVICE)\n",
    "            #logger.info(f\"X: {x[0][0]}\")\n",
    "            # Category Loss\n",
    "            #logger.info(f\"X: {x.size()}\")\n",
    "            for i in range(5):\n",
    "                x_t = x[i].float().to(DEVICE)\n",
    "                outputs, embeddings = model(x_t)\n",
    "                # Log details about the outputs\n",
    "                #logger.info(f\"Output type: {cls_o.logits.shape}\")\n",
    "                \n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                loss = criterion(outputs, y.long())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss[0] += loss.item()\n",
    "                epoch_loss[1] += x.size(0)\n",
    "\n",
    "                if (i_val + 1) % (len(train_loader) // 5) == 0:\n",
    "                    logger.info(\"[{}/{}]\".format(i_val + 1, len(train_loader)))\n",
    "            \n",
    "        scheduler.step()\n",
    "        logger.info(f'[EPOCH {epoch+1}] Avg. Loss: {epoch_loss[0] / epoch_loss[1]}')\n",
    "\n",
    "\n",
    "        #save checkpoint in a file\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            train_accuracy = evaluate(model, train_loader, DEVICE)\n",
    "            val_accuracy = evaluate(model, val_loader, DEVICE)\n",
    "            logger.info(f'[EPOCH {epoch+1}] Train Accuracy: {train_accuracy}')\n",
    "            logger.info(f'[EPOCH {epoch+1}] Val Accuracy: {val_accuracy}')\n",
    "            torch.save(model.state_dict(), f'./saved_models/LSTM_Emb_Classifier/final_LSTM_Emb_epoch_{epoch+1}.pth')\n",
    "        if (epoch+1) % STEP_SIZE == 0:\n",
    "            logger.info(f'Current LR: {scheduler.get_last_lr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:21,  9.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfeatures_file = \"saved_features/EMG_Emb_LSTM_25_dense_D1_train.pkl\"\\nwith open(features_file, \\'wb\\') as f:\\n    pickle.dump(data_to_save, f)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ActionNetEmgDataset('train', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2) # Inserisci il path al dataset di training\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) # Inserisci il dataloader per il training\n",
    "\n",
    "val_dataset = ActionNetEmgDataset('test', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "model.load_state_dict(torch.load(f'./saved_models/LSTM_Emb_Classifier/final_LSTM_Emb_epoch_40.pth'))\n",
    "model.eval()\n",
    "\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i_val,(x, y) in tqdm(enumerate(val_loader)):\n",
    "        x = x.reshape(1, 5, 25, -1)\n",
    "        x = x.permute(1, 0, 2, 3)\n",
    "        y = y.to(DEVICE)\n",
    "        for i in range(5):\n",
    "                x_t = x[i].float().to(DEVICE)\n",
    "                sample = {}\n",
    "                outputs, embeddings = model(x_t)\n",
    "                embeddings_list.append(embeddings.cpu())\n",
    "                labels_list.append(y.cpu())\n",
    "\n",
    "# Combina le features e le labels in una lista di dizionari\n",
    "data_to_save = [{'features': emb[0], 'labels': lbl} for emb, lbl in zip(embeddings_list, labels_list)]\n",
    "# print size of the data\n",
    "print(f\"Data size: {len(data_to_save)}\")\n",
    "# Salva i dati in un file pickle\n",
    "'''\n",
    "features_file = \"saved_features/EMG_Emb_LSTM_25_dense_D1_train.pkl\"\n",
    "with open(features_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "sample = data_to_save[0]\n",
    "print(f\"Data size: {sample['features'].size()}\")\n",
    "\n",
    "features_file = \"saved_features/EMG_Emb_LSTM_25_dense_D1_test.pkl\"\n",
    "with open(features_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGM Feature Extraction using Signal Properties such as Integrated EMG, Mean Squared Value, Variance, Root Mean Square, Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EMG_Feature_Extractor\n",
    "\n",
    "\n",
    "train_dataset = ActionNetEmgDataset('test', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2) # Inserisci il path al dataset di training\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) # Inserisci il dataloader per il training\n",
    "\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "for i_val,(x, y) in tqdm(enumerate(train_loader)):\n",
    "            x = x.reshape(1, 5, 25, -1)\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            y = y.to(DEVICE)\n",
    "            #logger.info(f\"X: {x[0][0]}\")\n",
    "            # Category Loss\n",
    "            #logger.info(f\"X: {x.size()}\")\n",
    "            for i in range(5):\n",
    "                sample = {}\n",
    "                x_t = x[i].float().to(DEVICE)\n",
    "                embeddings = EMG_Feature_Extractor(x_t[0])\n",
    "                embeddings_list.append(embeddings.cpu())  # Mantenere gli embeddings sulla CPU per salvare\n",
    "                labels_list.append(y.cpu())  # Mantenere le etichette sulla CPU per salvare\n",
    "\n",
    "data_to_save = [{'features': emb, 'labels': lbl} for emb, lbl in zip(embeddings_list, labels_list)]\n",
    "\n",
    "features_file = \"saved_features/EMG_Emb_Stat_25_dense_D1_test.pkl\"\n",
    "with open(features_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
