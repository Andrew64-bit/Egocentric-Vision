{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for EMG signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Emb_Classifier, EMG_Feature_Extractor\n",
    "from utils.logger import logger\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 10\n",
    "GAMMA = 0.1\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    logger.info(\"------ USING APPLE SILICON GPU ------\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri del modello\n",
    "input_dim = 8\n",
    "hidden_dim = 64\n",
    "embedding_dim = 32\n",
    "output_dim = 8  # Definisci il numero di classi\n",
    "\n",
    "# Inizializzazione del modello, della loss function e dell'ottimizzatore\n",
    "model = LSTM_Emb_Classifier(input_dim, hidden_dim, embedding_dim, output_dim)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_dataset = \"\" # Inserisci il path al dataset di training\n",
    "train_loader = \"\" # Inserisci il dataloader per il training\n",
    "\n",
    "val_dataset = \"\"\n",
    "val_loader = \"\"\n",
    "\n",
    "logger.info(f\"Model: {model}\")\n",
    "logger.info(f\"len train_dataset: {len(train_dataset)}\")\n",
    "logger.info(f\"len train_loader: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs, embeddings = model(x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = [0.0, 0]\n",
    "        for i_val,(x, y) in tqdm(enumerate(train_loader)):\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            #logger.info(f\"X: {x[0][0]}\")\n",
    "            # Category Loss\n",
    "            #logger.info(f\"X: {x.size()}\")\n",
    "\n",
    "            outputs, embeddings = model(x)\n",
    "            # Log details about the outputs\n",
    "            #logger.info(f\"Output type: {cls_o.logits.shape}\")\n",
    "                \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(outputs, y.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss[0] += loss.item()\n",
    "            epoch_loss[1] += x.size(0)\n",
    "            if (i_val + 1) % (len(train_loader) // 5) == 0:\n",
    "                logger.info(\"[{}/{}]\".format(i_val + 1, len(train_loader)))\n",
    "            \n",
    "        scheduler.step()\n",
    "        logger.info(f'[EPOCH {epoch+1}] Avg. Loss: {epoch_loss[0] / epoch_loss[1]}')\n",
    "\n",
    "\n",
    "        #save checkpoint in a file\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            train_accuracy = evaluate(model, train_loader, DEVICE)\n",
    "            val_accuracy = evaluate(model, val_loader, DEVICE)\n",
    "            logger.info(f'[EPOCH {epoch+1}] Train Accuracy: {train_accuracy}')\n",
    "            logger.info(f'[EPOCH {epoch+1}] Val Accuracy: {val_accuracy}')\n",
    "            torch.save(model.state_dict(), f'./saved_models/LSTM_Emb_Classifier/final_LSTM_Emb_epoch_{epoch+1}.pth')\n",
    "        if (epoch+1) % STEP_SIZE == 0:\n",
    "            logger.info(f'Current LR: {scheduler.get_last_lr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------TO ADJUST-------------\n",
    "extraction_dataset = \"\"\n",
    "extraction_loader = \"\"\n",
    "\n",
    "\n",
    "# Estrazione degli embeddings per i dati completi\n",
    "model.load_state_dict(torch.load(f'./saved_models/LSTM_Emb_Classifier/final_LSTM_Emb_epoch_50.pth'))\n",
    "with torch.no_grad():\n",
    "    all_embeddings = []\n",
    "    for i_val,(x, y) in tqdm(enumerate(extraction_loader)):\n",
    "        _, embeddings = model(x)\n",
    "        all_embeddings.append(embeddings)\n",
    "    all_embeddings = torch.cat(all_embeddings)\n",
    "\n",
    "print(all_embeddings.shape)  # Controlla la forma degli embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Not Neural Network Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
