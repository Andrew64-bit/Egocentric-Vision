{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for EMG signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using LSTM --->  TRAINING + EVALUATION + EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <0EB69795-4559-3C98-9EA1-35B6A988BB99> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <E4E2FFCA-031E-3974-A7B0-45408D7F4956> /Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import LSTM_Emb_Classifier, LSTM_Embedding_Classifier\n",
    "from utils.loaders import ActionNetEmgDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.logger import logger\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "from utils.loaders import FeaturesExtendedEMGDataset\n",
    "from models import MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training [ + EVALUATION ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 15:24:27 LOG INFO ------ USING APPLE SILICON GPU ------\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "LR = 0.0001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 1e-4\n",
    "STEP_SIZE = 20\n",
    "GAMMA = 0.1\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "    logger.info(\"------ USING APPLE SILICON GPU ------\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreavannozzi/GithubProjects/Multimodal-Egocentric-Action-Recognition/env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "2024-06-12 15:17:58 LOG INFO Model: LSTM_Embedding_Classifier(\n",
      "  (lstm): LSTM(16, 512, batch_first=True, dropout=0.5)\n",
      "  (attention): Attention(\n",
      "    (attention): Linear(in_features=512, out_features=1, bias=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=20, bias=True)\n",
      ")\n",
      "2024-06-12 15:17:58 LOG INFO len train_dataset: 1795\n",
      "2024-06-12 15:17:58 LOG INFO len train_loader: 56\n"
     ]
    }
   ],
   "source": [
    "# Parametri del modello\n",
    "input_dim = 16\n",
    "hidden_dim = 512\n",
    "embedding_dim = 1024\n",
    "output_dim = 20  # Definisci il numero di classi\n",
    "\n",
    "# Inizializzazione del modello, della loss function e dell'ottimizzatore\n",
    "model = LSTM_Embedding_Classifier(input_dim=input_dim, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_class=output_dim)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "train_dataset = ActionNetEmgDataset('train', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2) # Inserisci il path al dataset di training\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) # Inserisci il dataloader per il training\n",
    "\n",
    "val_dataset = ActionNetEmgDataset('test', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "logger.info(f\"Model: {model}\")\n",
    "logger.info(f\"len train_dataset: {len(train_dataset)}\")\n",
    "logger.info(f\"len train_loader: {len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x = x.reshape(BATCH_SIZE, 5, 25, -1)\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            for i in range(5):\n",
    "                x_t = x[i].float().to(device)\n",
    "                outputs, embeddings = model(x_t)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:01, 10.51it/s]2024-06-12 15:24:42 LOG INFO [11/56]\n",
      "2024-06-12 15:24:42 LOG INFO [11/56]\n",
      "2024-06-12 15:24:42 LOG INFO [11/56]\n",
      "2024-06-12 15:24:42 LOG INFO [11/56]\n",
      "2024-06-12 15:24:42 LOG INFO [11/56]\n",
      "21it [00:01, 18.40it/s]2024-06-12 15:24:43 LOG INFO [22/56]\n",
      "2024-06-12 15:24:43 LOG INFO [22/56]\n",
      "2024-06-12 15:24:43 LOG INFO [22/56]\n",
      "2024-06-12 15:24:43 LOG INFO [22/56]\n",
      "2024-06-12 15:24:43 LOG INFO [22/56]\n",
      "31it [00:02, 17.86it/s]2024-06-12 15:24:43 LOG INFO [33/56]\n",
      "2024-06-12 15:24:43 LOG INFO [33/56]\n",
      "2024-06-12 15:24:43 LOG INFO [33/56]\n",
      "2024-06-12 15:24:43 LOG INFO [33/56]\n",
      "2024-06-12 15:24:43 LOG INFO [33/56]\n",
      "43it [00:03, 19.92it/s]2024-06-12 15:24:44 LOG INFO [44/56]\n",
      "2024-06-12 15:24:44 LOG INFO [44/56]\n",
      "2024-06-12 15:24:44 LOG INFO [44/56]\n",
      "2024-06-12 15:24:44 LOG INFO [44/56]\n",
      "2024-06-12 15:24:44 LOG INFO [44/56]\n",
      "54it [00:03, 19.43it/s]2024-06-12 15:24:44 LOG INFO [55/56]\n",
      "2024-06-12 15:24:44 LOG INFO [55/56]\n",
      "2024-06-12 15:24:44 LOG INFO [55/56]\n",
      "2024-06-12 15:24:44 LOG INFO [55/56]\n",
      "2024-06-12 15:24:44 LOG INFO [55/56]\n",
      "56it [00:23,  2.36it/s]\n",
      "2024-06-12 15:25:05 LOG INFO [EPOCH 1] Avg. Loss: 0.19623040976268905\n",
      "10it [00:01, 12.43it/s]2024-06-12 15:25:12 LOG INFO [11/56]\n",
      "2024-06-12 15:25:12 LOG INFO [11/56]\n",
      "2024-06-12 15:25:12 LOG INFO [11/56]\n",
      "2024-06-12 15:25:12 LOG INFO [11/56]\n",
      "2024-06-12 15:25:12 LOG INFO [11/56]\n",
      "20it [00:01, 17.77it/s]2024-06-12 15:25:13 LOG INFO [22/56]\n",
      "2024-06-12 15:25:13 LOG INFO [22/56]\n",
      "2024-06-12 15:25:13 LOG INFO [22/56]\n",
      "2024-06-12 15:25:13 LOG INFO [22/56]\n",
      "2024-06-12 15:25:13 LOG INFO [22/56]\n",
      "32it [00:02, 18.66it/s]2024-06-12 15:25:13 LOG INFO [33/56]\n",
      "2024-06-12 15:25:13 LOG INFO [33/56]\n",
      "2024-06-12 15:25:13 LOG INFO [33/56]\n",
      "2024-06-12 15:25:13 LOG INFO [33/56]\n",
      "2024-06-12 15:25:13 LOG INFO [33/56]\n",
      "42it [00:02, 18.03it/s]2024-06-12 15:25:14 LOG INFO [44/56]\n",
      "2024-06-12 15:25:14 LOG INFO [44/56]\n",
      "2024-06-12 15:25:14 LOG INFO [44/56]\n",
      "2024-06-12 15:25:14 LOG INFO [44/56]\n",
      "2024-06-12 15:25:14 LOG INFO [44/56]\n",
      "52it [00:03, 17.68it/s]2024-06-12 15:25:15 LOG INFO [55/56]\n",
      "2024-06-12 15:25:15 LOG INFO [55/56]\n",
      "2024-06-12 15:25:15 LOG INFO [55/56]\n",
      "2024-06-12 15:25:15 LOG INFO [55/56]\n",
      "2024-06-12 15:25:15 LOG INFO [55/56]\n",
      "56it [00:23,  2.36it/s]\n",
      "2024-06-12 15:25:35 LOG INFO [EPOCH 2] Avg. Loss: 0.1967410861168589\n",
      "10it [00:01, 12.63it/s]2024-06-12 15:25:42 LOG INFO [11/56]\n",
      "2024-06-12 15:25:42 LOG INFO [11/56]\n",
      "2024-06-12 15:25:42 LOG INFO [11/56]\n",
      "2024-06-12 15:25:42 LOG INFO [11/56]\n",
      "2024-06-12 15:25:42 LOG INFO [11/56]\n",
      "21it [00:01, 18.38it/s]2024-06-12 15:25:43 LOG INFO [22/56]\n",
      "2024-06-12 15:25:43 LOG INFO [22/56]\n",
      "2024-06-12 15:25:43 LOG INFO [22/56]\n",
      "2024-06-12 15:25:43 LOG INFO [22/56]\n",
      "2024-06-12 15:25:43 LOG INFO [22/56]\n",
      "31it [00:02, 19.44it/s]2024-06-12 15:25:43 LOG INFO [33/56]\n",
      "2024-06-12 15:25:43 LOG INFO [33/56]\n",
      "2024-06-12 15:25:43 LOG INFO [33/56]\n",
      "2024-06-12 15:25:43 LOG INFO [33/56]\n",
      "2024-06-12 15:25:43 LOG INFO [33/56]\n",
      "41it [00:02, 18.80it/s]2024-06-12 15:25:44 LOG INFO [44/56]\n",
      "2024-06-12 15:25:44 LOG INFO [44/56]\n",
      "2024-06-12 15:25:44 LOG INFO [44/56]\n",
      "2024-06-12 15:25:44 LOG INFO [44/56]\n",
      "2024-06-12 15:25:44 LOG INFO [44/56]\n",
      "53it [00:03, 21.60it/s]2024-06-12 15:25:44 LOG INFO [55/56]\n",
      "2024-06-12 15:25:44 LOG INFO [55/56]\n",
      "2024-06-12 15:25:44 LOG INFO [55/56]\n",
      "2024-06-12 15:25:44 LOG INFO [55/56]\n",
      "2024-06-12 15:25:44 LOG INFO [55/56]\n",
      "56it [00:23,  2.39it/s]\n",
      "2024-06-12 15:26:05 LOG INFO [EPOCH 3] Avg. Loss: 0.19467871001788548\n",
      "9it [00:01, 11.73it/s]2024-06-12 15:26:12 LOG INFO [11/56]\n",
      "2024-06-12 15:26:12 LOG INFO [11/56]\n",
      "2024-06-12 15:26:12 LOG INFO [11/56]\n",
      "2024-06-12 15:26:12 LOG INFO [11/56]\n",
      "2024-06-12 15:26:12 LOG INFO [11/56]\n",
      "20it [00:01, 18.12it/s]2024-06-12 15:26:13 LOG INFO [22/56]\n",
      "2024-06-12 15:26:13 LOG INFO [22/56]\n",
      "2024-06-12 15:26:13 LOG INFO [22/56]\n",
      "2024-06-12 15:26:13 LOG INFO [22/56]\n",
      "2024-06-12 15:26:13 LOG INFO [22/56]\n",
      "31it [00:02, 20.48it/s]2024-06-12 15:26:13 LOG INFO [33/56]\n",
      "2024-06-12 15:26:13 LOG INFO [33/56]\n",
      "2024-06-12 15:26:13 LOG INFO [33/56]\n",
      "2024-06-12 15:26:13 LOG INFO [33/56]\n",
      "2024-06-12 15:26:13 LOG INFO [33/56]\n",
      "43it [00:02, 20.98it/s]2024-06-12 15:26:14 LOG INFO [44/56]\n",
      "2024-06-12 15:26:14 LOG INFO [44/56]\n",
      "2024-06-12 15:26:14 LOG INFO [44/56]\n",
      "2024-06-12 15:26:14 LOG INFO [44/56]\n",
      "2024-06-12 15:26:14 LOG INFO [44/56]\n",
      "52it [00:03, 21.62it/s]2024-06-12 15:26:14 LOG INFO [55/56]\n",
      "2024-06-12 15:26:14 LOG INFO [55/56]\n",
      "2024-06-12 15:26:14 LOG INFO [55/56]\n",
      "2024-06-12 15:26:14 LOG INFO [55/56]\n",
      "2024-06-12 15:26:14 LOG INFO [55/56]\n",
      "56it [00:23,  2.40it/s]\n",
      "2024-06-12 15:26:34 LOG INFO [EPOCH 4] Avg. Loss: 0.1900828304886818\n",
      "10it [00:01, 12.88it/s]2024-06-12 15:26:42 LOG INFO [11/56]\n",
      "2024-06-12 15:26:42 LOG INFO [11/56]\n",
      "2024-06-12 15:26:42 LOG INFO [11/56]\n",
      "2024-06-12 15:26:42 LOG INFO [11/56]\n",
      "2024-06-12 15:26:42 LOG INFO [11/56]\n",
      "20it [00:01, 17.83it/s]2024-06-12 15:26:42 LOG INFO [22/56]\n",
      "2024-06-12 15:26:42 LOG INFO [22/56]\n",
      "2024-06-12 15:26:42 LOG INFO [22/56]\n",
      "2024-06-12 15:26:42 LOG INFO [22/56]\n",
      "2024-06-12 15:26:42 LOG INFO [22/56]\n",
      "32it [00:02, 18.95it/s]2024-06-12 15:26:43 LOG INFO [33/56]\n",
      "2024-06-12 15:26:43 LOG INFO [33/56]\n",
      "2024-06-12 15:26:43 LOG INFO [33/56]\n",
      "2024-06-12 15:26:43 LOG INFO [33/56]\n",
      "2024-06-12 15:26:43 LOG INFO [33/56]\n",
      "42it [00:02, 20.09it/s]2024-06-12 15:26:43 LOG INFO [44/56]\n",
      "2024-06-12 15:26:43 LOG INFO [44/56]\n",
      "2024-06-12 15:26:43 LOG INFO [44/56]\n",
      "2024-06-12 15:26:43 LOG INFO [44/56]\n",
      "2024-06-12 15:26:43 LOG INFO [44/56]\n",
      "52it [00:03, 20.15it/s]2024-06-12 15:26:44 LOG INFO [55/56]\n",
      "2024-06-12 15:26:44 LOG INFO [55/56]\n",
      "2024-06-12 15:26:44 LOG INFO [55/56]\n",
      "2024-06-12 15:26:44 LOG INFO [55/56]\n",
      "2024-06-12 15:26:44 LOG INFO [55/56]\n",
      "56it [00:23,  2.39it/s]\n",
      "2024-06-12 15:27:04 LOG INFO [EPOCH 5] Avg. Loss: 0.18975604348949024\n",
      "10it [00:01, 12.50it/s]2024-06-12 15:27:11 LOG INFO [11/56]\n",
      "2024-06-12 15:27:11 LOG INFO [11/56]\n",
      "2024-06-12 15:27:11 LOG INFO [11/56]\n",
      "2024-06-12 15:27:11 LOG INFO [11/56]\n",
      "2024-06-12 15:27:11 LOG INFO [11/56]\n",
      "20it [00:01, 16.78it/s]2024-06-12 15:27:12 LOG INFO [22/56]\n",
      "2024-06-12 15:27:12 LOG INFO [22/56]\n",
      "2024-06-12 15:27:12 LOG INFO [22/56]\n",
      "2024-06-12 15:27:12 LOG INFO [22/56]\n",
      "2024-06-12 15:27:12 LOG INFO [22/56]\n",
      "32it [00:02, 20.97it/s]2024-06-12 15:27:13 LOG INFO [33/56]\n",
      "2024-06-12 15:27:13 LOG INFO [33/56]\n",
      "2024-06-12 15:27:13 LOG INFO [33/56]\n",
      "2024-06-12 15:27:13 LOG INFO [33/56]\n",
      "2024-06-12 15:27:13 LOG INFO [33/56]\n",
      "41it [00:02, 21.42it/s]2024-06-12 15:27:13 LOG INFO [44/56]\n",
      "2024-06-12 15:27:13 LOG INFO [44/56]\n",
      "2024-06-12 15:27:13 LOG INFO [44/56]\n",
      "2024-06-12 15:27:13 LOG INFO [44/56]\n",
      "2024-06-12 15:27:13 LOG INFO [44/56]\n",
      "53it [00:03, 21.39it/s]2024-06-12 15:27:14 LOG INFO [55/56]\n",
      "2024-06-12 15:27:14 LOG INFO [55/56]\n",
      "2024-06-12 15:27:14 LOG INFO [55/56]\n",
      "2024-06-12 15:27:14 LOG INFO [55/56]\n",
      "2024-06-12 15:27:14 LOG INFO [55/56]\n",
      "56it [00:23,  2.39it/s]\n",
      "2024-06-12 15:27:34 LOG INFO [EPOCH 6] Avg. Loss: 0.18686436193329947\n",
      "10it [00:01, 12.54it/s]2024-06-12 15:27:41 LOG INFO [11/56]\n",
      "2024-06-12 15:27:41 LOG INFO [11/56]\n",
      "2024-06-12 15:27:41 LOG INFO [11/56]\n",
      "2024-06-12 15:27:41 LOG INFO [11/56]\n",
      "2024-06-12 15:27:41 LOG INFO [11/56]\n",
      "19it [00:01, 17.81it/s]2024-06-12 15:27:42 LOG INFO [22/56]\n",
      "2024-06-12 15:27:42 LOG INFO [22/56]\n",
      "2024-06-12 15:27:42 LOG INFO [22/56]\n",
      "2024-06-12 15:27:42 LOG INFO [22/56]\n",
      "2024-06-12 15:27:42 LOG INFO [22/56]\n",
      "31it [00:02, 18.48it/s]2024-06-12 15:27:42 LOG INFO [33/56]\n",
      "2024-06-12 15:27:42 LOG INFO [33/56]\n",
      "2024-06-12 15:27:42 LOG INFO [33/56]\n",
      "2024-06-12 15:27:42 LOG INFO [33/56]\n",
      "2024-06-12 15:27:42 LOG INFO [33/56]\n",
      "42it [00:02, 20.55it/s]2024-06-12 15:27:43 LOG INFO [44/56]\n",
      "2024-06-12 15:27:43 LOG INFO [44/56]\n",
      "2024-06-12 15:27:43 LOG INFO [44/56]\n",
      "2024-06-12 15:27:43 LOG INFO [44/56]\n",
      "2024-06-12 15:27:43 LOG INFO [44/56]\n",
      "54it [00:03, 21.02it/s]2024-06-12 15:27:43 LOG INFO [55/56]\n",
      "2024-06-12 15:27:43 LOG INFO [55/56]\n",
      "2024-06-12 15:27:43 LOG INFO [55/56]\n",
      "2024-06-12 15:27:43 LOG INFO [55/56]\n",
      "2024-06-12 15:27:43 LOG INFO [55/56]\n",
      "56it [00:23,  2.38it/s]\n",
      "2024-06-12 15:28:03 LOG INFO [EPOCH 7] Avg. Loss: 0.18805201062134333\n",
      "9it [00:01, 10.65it/s]2024-06-12 15:28:11 LOG INFO [11/56]\n",
      "2024-06-12 15:28:11 LOG INFO [11/56]\n",
      "2024-06-12 15:28:11 LOG INFO [11/56]\n",
      "2024-06-12 15:28:11 LOG INFO [11/56]\n",
      "2024-06-12 15:28:11 LOG INFO [11/56]\n",
      "19it [00:01, 18.05it/s]2024-06-12 15:28:11 LOG INFO [22/56]\n",
      "2024-06-12 15:28:11 LOG INFO [22/56]\n",
      "2024-06-12 15:28:11 LOG INFO [22/56]\n",
      "2024-06-12 15:28:11 LOG INFO [22/56]\n",
      "2024-06-12 15:28:11 LOG INFO [22/56]\n",
      "32it [00:02, 19.79it/s]2024-06-12 15:28:12 LOG INFO [33/56]\n",
      "2024-06-12 15:28:12 LOG INFO [33/56]\n",
      "2024-06-12 15:28:12 LOG INFO [33/56]\n",
      "2024-06-12 15:28:12 LOG INFO [33/56]\n",
      "2024-06-12 15:28:12 LOG INFO [33/56]\n",
      "42it [00:02, 18.22it/s]2024-06-12 15:28:13 LOG INFO [44/56]\n",
      "2024-06-12 15:28:13 LOG INFO [44/56]\n",
      "2024-06-12 15:28:13 LOG INFO [44/56]\n",
      "2024-06-12 15:28:13 LOG INFO [44/56]\n",
      "2024-06-12 15:28:13 LOG INFO [44/56]\n",
      "54it [00:03, 23.92it/s]2024-06-12 15:28:13 LOG INFO [55/56]\n",
      "2024-06-12 15:28:13 LOG INFO [55/56]\n",
      "2024-06-12 15:28:13 LOG INFO [55/56]\n",
      "2024-06-12 15:28:13 LOG INFO [55/56]\n",
      "2024-06-12 15:28:13 LOG INFO [55/56]\n",
      "56it [00:23,  2.38it/s]\n",
      "2024-06-12 15:28:33 LOG INFO [EPOCH 8] Avg. Loss: 0.17442652866244315\n",
      "9it [00:01, 11.57it/s]2024-06-12 15:28:41 LOG INFO [11/56]\n",
      "2024-06-12 15:28:41 LOG INFO [11/56]\n",
      "2024-06-12 15:28:41 LOG INFO [11/56]\n",
      "2024-06-12 15:28:41 LOG INFO [11/56]\n",
      "2024-06-12 15:28:41 LOG INFO [11/56]\n",
      "21it [00:01, 19.31it/s]2024-06-12 15:28:41 LOG INFO [22/56]\n",
      "2024-06-12 15:28:41 LOG INFO [22/56]\n",
      "2024-06-12 15:28:41 LOG INFO [22/56]\n",
      "2024-06-12 15:28:41 LOG INFO [22/56]\n",
      "2024-06-12 15:28:41 LOG INFO [22/56]\n",
      "30it [00:02, 20.65it/s]2024-06-12 15:28:42 LOG INFO [33/56]\n",
      "2024-06-12 15:28:42 LOG INFO [33/56]\n",
      "2024-06-12 15:28:42 LOG INFO [33/56]\n",
      "2024-06-12 15:28:42 LOG INFO [33/56]\n",
      "2024-06-12 15:28:42 LOG INFO [33/56]\n",
      "42it [00:02, 21.32it/s]2024-06-12 15:28:42 LOG INFO [44/56]\n",
      "2024-06-12 15:28:42 LOG INFO [44/56]\n",
      "2024-06-12 15:28:42 LOG INFO [44/56]\n",
      "2024-06-12 15:28:42 LOG INFO [44/56]\n",
      "2024-06-12 15:28:42 LOG INFO [44/56]\n",
      "52it [00:03, 18.24it/s]2024-06-12 15:28:43 LOG INFO [55/56]\n",
      "2024-06-12 15:28:43 LOG INFO [55/56]\n",
      "2024-06-12 15:28:43 LOG INFO [55/56]\n",
      "2024-06-12 15:28:43 LOG INFO [55/56]\n",
      "2024-06-12 15:28:43 LOG INFO [55/56]\n",
      "56it [00:23,  2.38it/s]\n",
      "2024-06-12 15:29:03 LOG INFO [EPOCH 9] Avg. Loss: 0.17921735789094653\n",
      "10it [00:01, 11.74it/s]2024-06-12 15:29:11 LOG INFO [11/56]\n",
      "2024-06-12 15:29:11 LOG INFO [11/56]\n",
      "2024-06-12 15:29:11 LOG INFO [11/56]\n",
      "2024-06-12 15:29:11 LOG INFO [11/56]\n",
      "2024-06-12 15:29:11 LOG INFO [11/56]\n",
      "19it [00:01, 17.96it/s]2024-06-12 15:29:11 LOG INFO [22/56]\n",
      "2024-06-12 15:29:11 LOG INFO [22/56]\n",
      "2024-06-12 15:29:11 LOG INFO [22/56]\n",
      "2024-06-12 15:29:11 LOG INFO [22/56]\n",
      "2024-06-12 15:29:11 LOG INFO [22/56]\n",
      "31it [00:02, 20.24it/s]2024-06-12 15:29:12 LOG INFO [33/56]\n",
      "2024-06-12 15:29:12 LOG INFO [33/56]\n",
      "2024-06-12 15:29:12 LOG INFO [33/56]\n",
      "2024-06-12 15:29:12 LOG INFO [33/56]\n",
      "2024-06-12 15:29:12 LOG INFO [33/56]\n",
      "43it [00:02, 20.77it/s]2024-06-12 15:29:12 LOG INFO [44/56]\n",
      "2024-06-12 15:29:12 LOG INFO [44/56]\n",
      "2024-06-12 15:29:12 LOG INFO [44/56]\n",
      "2024-06-12 15:29:12 LOG INFO [44/56]\n",
      "2024-06-12 15:29:12 LOG INFO [44/56]\n",
      "52it [00:03, 21.34it/s]2024-06-12 15:29:13 LOG INFO [55/56]\n",
      "2024-06-12 15:29:13 LOG INFO [55/56]\n",
      "2024-06-12 15:29:13 LOG INFO [55/56]\n",
      "2024-06-12 15:29:13 LOG INFO [55/56]\n",
      "2024-06-12 15:29:13 LOG INFO [55/56]\n",
      "56it [00:23,  2.38it/s]\n",
      "2024-06-12 15:29:33 LOG INFO [EPOCH 10] Avg. Loss: 0.1775520724909646\n",
      "2024-06-12 15:30:26 LOG INFO [EPOCH 10] Train Accuracy: 0.7670758928571428\n",
      "2024-06-12 15:30:26 LOG INFO [EPOCH 10] Val Accuracy: 0.4510416666666667\n",
      "10it [00:01, 11.11it/s]2024-06-12 15:30:34 LOG INFO [11/56]\n",
      "2024-06-12 15:30:34 LOG INFO [11/56]\n",
      "2024-06-12 15:30:34 LOG INFO [11/56]\n",
      "2024-06-12 15:30:34 LOG INFO [11/56]\n",
      "2024-06-12 15:30:34 LOG INFO [11/56]\n",
      "20it [00:01, 17.23it/s]2024-06-12 15:30:35 LOG INFO [22/56]\n",
      "2024-06-12 15:30:35 LOG INFO [22/56]\n",
      "2024-06-12 15:30:35 LOG INFO [22/56]\n",
      "2024-06-12 15:30:35 LOG INFO [22/56]\n",
      "2024-06-12 15:30:35 LOG INFO [22/56]\n",
      "32it [00:02, 20.84it/s]2024-06-12 15:30:35 LOG INFO [33/56]\n",
      "2024-06-12 15:30:35 LOG INFO [33/56]\n",
      "2024-06-12 15:30:35 LOG INFO [33/56]\n",
      "2024-06-12 15:30:35 LOG INFO [33/56]\n",
      "2024-06-12 15:30:35 LOG INFO [33/56]\n",
      "41it [00:02, 17.46it/s]2024-06-12 15:30:36 LOG INFO [44/56]\n",
      "2024-06-12 15:30:36 LOG INFO [44/56]\n",
      "2024-06-12 15:30:36 LOG INFO [44/56]\n",
      "2024-06-12 15:30:36 LOG INFO [44/56]\n",
      "2024-06-12 15:30:36 LOG INFO [44/56]\n",
      "54it [00:03, 20.41it/s]2024-06-12 15:30:36 LOG INFO [55/56]\n",
      "2024-06-12 15:30:36 LOG INFO [55/56]\n",
      "2024-06-12 15:30:36 LOG INFO [55/56]\n",
      "2024-06-12 15:30:36 LOG INFO [55/56]\n",
      "2024-06-12 15:30:36 LOG INFO [55/56]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/00/ActionNet_Feature_Extractor/model40.pth'))\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = [0.0, 0]\n",
    "        for i_val,(x, y) in tqdm(enumerate(train_loader)):\n",
    "            x = x.reshape(BATCH_SIZE, 5, 25, -1)\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            y = y.to(DEVICE)\n",
    "            #logger.info(f\"X: {x[0][0]}\")\n",
    "            # Category Loss\n",
    "            #logger.info(f\"X: {x.size()}\")\n",
    "            for i in range(5):\n",
    "                x_t = x[i].float().to(DEVICE)\n",
    "                outputs, embeddings = model(x_t)\n",
    "                # Log details about the outputs\n",
    "                #logger.info(f\"Output type: {cls_o.logits.shape}\")\n",
    "                \n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                loss = criterion(outputs, y.long())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss[0] += loss.item()\n",
    "                epoch_loss[1] += x.size(0)\n",
    "\n",
    "                if (i_val + 1) % (len(train_loader) // 5) == 0:\n",
    "                    logger.info(\"[{}/{}]\".format(i_val + 1, len(train_loader)))\n",
    "            \n",
    "        scheduler.step()\n",
    "        logger.info(f'[EPOCH {epoch+1}] Avg. Loss: {epoch_loss[0] / epoch_loss[1]}')\n",
    "\n",
    "\n",
    "        #save checkpoint in a file\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            train_accuracy = evaluate(model, train_loader, DEVICE)\n",
    "            val_accuracy = evaluate(model, val_loader, DEVICE)\n",
    "            logger.info(f'[EPOCH {epoch+1}] Train Accuracy: {train_accuracy}')\n",
    "            logger.info(f'[EPOCH {epoch+1}] Val Accuracy: {val_accuracy}')\n",
    "            torch.save(model.state_dict(), f'./saved_models/00/ActionNet_Feature_Extractor/model{epoch+1}.pth')\n",
    "        if (epoch+1) % STEP_SIZE == 0:\n",
    "            logger.info(f'Current LR: {scheduler.get_last_lr()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:21,  9.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfeatures_file = \"saved_features/EMG_Emb_LSTM_25_dense_D1_train.pkl\"\\nwith open(features_file, \\'wb\\') as f:\\n    pickle.dump(data_to_save, f)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ActionNetEmgDataset('train', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2) # Inserisci il path al dataset di training\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) # Inserisci il dataloader per il training\n",
    "\n",
    "val_dataset = ActionNetEmgDataset('test', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "\n",
    "model = LSTM_Emb_Classifier(input_dim=input_dim, hidden_dim=hidden_dim, embedding_dim=embedding_dim, num_class=output_dim)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(f'./saved_models/LSTM_Emb_Classifier/NEW_final_LSTM_Emb_25_epoch_10.pth'))\n",
    "model.eval()\n",
    "\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i_val,(x, y) in tqdm(enumerate(val_loader)):\n",
    "        x = x.reshape(1, 5, 25, -1)\n",
    "        x = x.permute(1, 0, 2, 3)\n",
    "        y = y.to(DEVICE)\n",
    "        for i in range(5):\n",
    "                x_t = x[i].float().to(DEVICE)\n",
    "                sample = {}\n",
    "                outputs, embeddings = model(x_t)\n",
    "                embeddings_list.append(embeddings.cpu())\n",
    "                labels_list.append(y.cpu())\n",
    "\n",
    "# Combina le features e le labels in una lista di dizionari\n",
    "data_to_save = [{'features': emb[0], 'labels': lbl} for emb, lbl in zip(embeddings_list, labels_list)]\n",
    "# print size of the data\n",
    "print(f\"Data size: {len(data_to_save)}\")\n",
    "# Salva i dati in un file pickle\n",
    "\n",
    "features_file = \"saved_features/EMG_Emb_LSTM_25_dense_D1_train.pkl\"\n",
    "with open(features_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "sample = data_to_save[0]\n",
    "print(f\"Data size: {sample['features'].size()}\")\n",
    "\n",
    "features_file = \"saved_features/NEW_EMG_Emb_LSTM_25_dense_D1_train.pkl\"\n",
    "with open(features_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGM Feature Extraction using Signal Properties such as Integrated EMG, Mean Squared Value, Variance, Root Mean Square, Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EMG_Feature_Extractor\n",
    "\n",
    "\n",
    "train_dataset = ActionNetEmgDataset('test', 25, 5, True, './action-net', \"./action-net/saved_emg\", 2) # Inserisci il path al dataset di training\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True, drop_last=True) # Inserisci il dataloader per il training\n",
    "\n",
    "embeddings_list = []\n",
    "labels_list = []\n",
    "\n",
    "for i_val,(x, y) in tqdm(enumerate(train_loader)):\n",
    "            x = x.reshape(1, 5, 25, -1)\n",
    "            x = x.permute(1, 0, 2, 3)\n",
    "            y = y.to(DEVICE)\n",
    "            #logger.info(f\"X: {x[0][0]}\")\n",
    "            # Category Loss\n",
    "            #logger.info(f\"X: {x.size()}\")\n",
    "            for i in range(5):\n",
    "                sample = {}\n",
    "                x_t = x[i].float().to(DEVICE)\n",
    "                embeddings = EMG_Feature_Extractor(x_t[0])\n",
    "                embeddings_list.append(embeddings.cpu())  # Mantenere gli embeddings sulla CPU per salvare\n",
    "                labels_list.append(y.cpu())  # Mantenere le etichette sulla CPU per salvare\n",
    "\n",
    "data_to_save = [{'features': emb, 'labels': lbl} for emb, lbl in zip(embeddings_list, labels_list)]\n",
    "\n",
    "features_file = \"saved_features/EMG_Emb_Stat_25_dense_D1_test.pkl\"\n",
    "with open(features_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPIC Kitchens EMG Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.loaders import FeaturesExtendedDataset, FeaturesExtendedEMGDataset, ActionNetEmgRgbDataset\n",
    "from models import I3D\n",
    "from models import EMG_Feature_Extractor\n",
    "from utils.args import args\n",
    "from omegaconf import OmegaConf\n",
    "import tqdm\n",
    "import pickle\n",
    "from utils.loaders import FeaturesDataset\n",
    "\n",
    "from models import FC_VAE, LSTM_Emb_Classifier, EMG_Feature_Extractor\n",
    "from train_vae import train_tuning, evaluate_tuning\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"------ USING APPLE SILICON GPU ------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1543\n",
      "Test dataset size: 435\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "train_dataset = FeaturesDataset(\"./saved_features/saved_feat_I3D_25_dense_D1\",'train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "test_dataset = FeaturesDataset(\"./saved_features/saved_feat_I3D_25_dense_D1\",'test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings train size: 1543\n",
      "Labels train size: 1543\n",
      "Embeddings test size: 435\n",
      "Labels test size: 435\n"
     ]
    }
   ],
   "source": [
    "# Inizializzazione del modello, della loss function e dell'ottimizzatore\n",
    "model_finetune = FC_VAE(dim_input=1024, nz=64,device='cpu', dim_output=1024)\n",
    "model_finetune.to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load('./saved_models/VAE_Fine_Tuninng/VAE_RGB_to_EMG_LSTM_epoch_50.pth', map_location=DEVICE)\n",
    "model_finetune.load_state_dict(checkpoint)\n",
    "\n",
    "model_finetune.eval()\n",
    "\n",
    "emb_train = []\n",
    "lab_train = []\n",
    "emb_test = []\n",
    "lab_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for (idx_train ,(x_train, y_train)) in enumerate(train_loader):\n",
    "        x_train = x_train.reshape(5, 1, 1024)\n",
    "        x_train_t = x_train.float().to(DEVICE)\n",
    "        train_clips = []\n",
    "        for i in range(5):\n",
    "            embeddings_train, latents, mu, logvar = model_finetune(x_train_t[i])\n",
    "            train_clips.append(embeddings_train[0].cpu().detach().numpy())\n",
    "\n",
    "        lab_train.append(y_train[0].cpu().detach().numpy())\n",
    "        emb_train.append(train_clips)\n",
    "\n",
    "    for (idx_test ,(x_test, y_test)) in enumerate(test_loader):\n",
    "        x_test = x_test.reshape(5, 1, 1024)\n",
    "        x_test_t = x_test.float().to(DEVICE)\n",
    "        test_clips = []\n",
    "        for i in range(5):\n",
    "            embeddings_test, latents, mu, logvar = model_finetune(x_test_t[i])\n",
    "            test_clips.append(embeddings_test[0].cpu().detach().numpy())\n",
    "        lab_test.append(y_test[0].cpu().detach().numpy())\n",
    "        emb_test.append(test_clips)\n",
    "\n",
    "                            \n",
    "\n",
    "print(f\"Embeddings train size: {len(emb_train)}\")\n",
    "print(f\"Labels train size: {len(lab_train)}\")\n",
    "print(f\"Embeddings test size: {len(emb_test)}\")\n",
    "print(f\"Labels test size: {len(lab_test)}\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(3),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(7),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(1),\n",
       " array(0),\n",
       " array(5),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(5),\n",
       " array(5),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(5),\n",
       " array(5),\n",
       " array(7),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(7),\n",
       " array(1),\n",
       " array(7),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(7),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(1),\n",
       " array(3),\n",
       " array(4),\n",
       " array(4),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(3),\n",
       " array(3),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(3),\n",
       " array(0),\n",
       " array(2),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(4),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(5),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(3),\n",
       " array(4),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(5),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(7),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(2),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(5),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(3),\n",
       " array(0),\n",
       " array(0),\n",
       " array(7),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(5),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(3),\n",
       " array(5),\n",
       " array(5),\n",
       " array(0),\n",
       " array(5),\n",
       " array(0),\n",
       " array(2),\n",
       " array(4),\n",
       " array(4),\n",
       " array(3),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(4),\n",
       " array(3),\n",
       " array(4),\n",
       " array(0),\n",
       " array(5),\n",
       " array(5),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(6),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(5),\n",
       " array(1),\n",
       " array(5),\n",
       " array(1),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(5),\n",
       " array(5),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(7),\n",
       " array(6),\n",
       " array(7),\n",
       " array(1),\n",
       " array(6),\n",
       " array(7),\n",
       " array(6),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(5),\n",
       " array(5),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(3),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(5),\n",
       " array(7),\n",
       " array(5),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(6),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(6),\n",
       " array(6),\n",
       " array(1),\n",
       " array(7),\n",
       " array(6),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(2),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(5),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(2),\n",
       " array(3),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(7),\n",
       " array(7),\n",
       " array(3),\n",
       " array(7),\n",
       " array(6),\n",
       " array(7),\n",
       " array(0),\n",
       " array(6),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(4),\n",
       " array(0),\n",
       " array(0),\n",
       " array(2),\n",
       " array(4),\n",
       " array(3),\n",
       " array(4),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(4),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(3),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(4),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(3),\n",
       " array(1),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(5),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(4),\n",
       " array(3),\n",
       " array(3),\n",
       " array(4),\n",
       " array(2),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(4),\n",
       " array(1),\n",
       " array(3),\n",
       " array(4),\n",
       " array(3),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(2),\n",
       " array(7),\n",
       " array(3),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(4),\n",
       " array(0),\n",
       " array(0),\n",
       " array(5),\n",
       " array(0),\n",
       " array(5),\n",
       " array(5),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(7),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(3),\n",
       " array(3),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(7),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(2),\n",
       " array(3),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(3),\n",
       " array(2),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(3),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(3),\n",
       " array(4),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(5),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(7),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(5),\n",
       " array(5),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(5),\n",
       " array(5),\n",
       " array(0),\n",
       " array(1),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(4),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(2),\n",
       " array(2),\n",
       " array(1),\n",
       " array(4),\n",
       " array(1),\n",
       " array(3),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(0),\n",
       " array(4),\n",
       " array(1),\n",
       " array(1),\n",
       " array(3),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(2),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(2),\n",
       " array(0),\n",
       " array(7),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(7),\n",
       " array(7),\n",
       " array(7),\n",
       " array(6),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(7),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(2),\n",
       " array(7),\n",
       " array(1),\n",
       " array(3),\n",
       " array(6),\n",
       " array(2),\n",
       " array(0),\n",
       " array(0),\n",
       " array(1),\n",
       " array(2),\n",
       " array(3),\n",
       " array(1),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(4),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(7),\n",
       " array(3),\n",
       " array(0),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(4),\n",
       " array(0),\n",
       " array(4),\n",
       " array(6),\n",
       " array(6),\n",
       " array(2),\n",
       " array(1),\n",
       " array(0),\n",
       " array(7),\n",
       " array(3),\n",
       " array(1),\n",
       " array(2),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(1),\n",
       " array(6),\n",
       " array(2),\n",
       " array(7),\n",
       " array(3),\n",
       " array(2),\n",
       " array(1),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(6),\n",
       " array(2),\n",
       " array(6),\n",
       " array(7),\n",
       " array(6),\n",
       " array(7),\n",
       " array(3),\n",
       " array(1),\n",
       " array(6),\n",
       " array(6),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(7),\n",
       " array(6),\n",
       " array(0),\n",
       " array(0),\n",
       " array(7),\n",
       " array(6),\n",
       " array(6),\n",
       " array(7),\n",
       " array(7),\n",
       " array(1),\n",
       " array(1),\n",
       " array(4),\n",
       " array(2),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(5),\n",
       " array(0),\n",
       " array(5),\n",
       " array(2),\n",
       " array(7),\n",
       " array(7),\n",
       " array(2),\n",
       " array(0),\n",
       " array(1),\n",
       " array(1),\n",
       " array(1),\n",
       " array(4),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(3),\n",
       " array(2),\n",
       " array(0),\n",
       " array(2),\n",
       " array(3),\n",
       " array(3),\n",
       " array(1),\n",
       " array(2),\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"saved_features/EPIC_KITCHENS_EMG_LSTM_25_dense_D1_train.pkl\"\n",
    "test_file = \"saved_features/EPIC_KITCHENS_EMG_LSTM_25_dense_D1_test.pkl\"\n",
    "\n",
    "data_to_save_train = [{'features': emb, 'labels': lbl} for emb, lbl in zip(emb_train, lab_train)]\n",
    "data_to_save_test = [{'features': emb, 'labels': lbl} for emb, lbl in zip(emb_test, lab_test)]\n",
    "\n",
    "with open(train_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save_train, f)\n",
    "\n",
    "with open(test_file, 'wb') as f:\n",
    "    pickle.dump(data_to_save_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
